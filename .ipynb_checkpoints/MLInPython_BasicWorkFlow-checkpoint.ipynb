{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQc-yqOSALtF"
   },
   "source": [
    "# A Basic Work Flow of Machine Learning in Python\n",
    "\n",
    "*Xianjun Geng, Spring 2023*\n",
    "\n",
    "We will together walk through a very basic supervised learning process using the Python programming language. \n",
    "\n",
    "Important: This lecture is prepared for students who are *already* experienced with predictive analytics using R. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: what is machine learning?\n",
    "\n",
    "Simply put, machine learning (ML) attempts to learn **patterns** -- a.k.a. things that repeat themselves -- from past data. *The repetition nature of a found pattern gives us hope that the pattern will again repeat itself in the future*, thus we can use machine learning to *predict*.\n",
    "\n",
    "We discover patterns using a wide range of mathematical models, a.k.a. **learning algorithms**, that researchers from many domains such as mathematics, computer science, statistics and the business world invented. \n",
    "+ This also explains why we have many names for (largely) the same thing, e.g., data science, KDD (Knowledge Discovery in Databases), machine learning, predictive inference, business intelligence, business analytics. \n",
    "\n",
    "There are three main caterories of machine learning: \n",
    "+ **supervised learning** (this is what we'll mainly focus on in this course)\n",
    "+ **unsupervised learning** (when data has inputs but no output, examples include clustering, anomaly/outlier detection, association rules ...)\n",
    "+ **reenforcement learning** (Not required for this course. See [this excellent short article](https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html) if interested. Famous for beating human pros in board/poker games, e.g. [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far) and [ReBel](https://ai.facebook.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/).)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "Also called **predictive analytics**, supervised learning tries to build a **function** (a.k.a. an **algorithm**) that *maps a set of inputs to an output using past data of inputs-output pairs*. We assume that this found algorithm/function holds for both past and future data, and thus we can apply this function to future inputs to *predict* the corresponding future output.\n",
    " \n",
    "![Spam filter](image_spam_filter.png)\n",
    "\n",
    "Many names for inputs and output:\n",
    "+ inputs are also called: predictors, independent variables, **features**, input columns, (in Scikit-Learn) X\n",
    "+ output is also called: prediction, dependent variable, label, **target**, output column, (in Scikit-Learn) y\n",
    "  + guess what \"labeled data\" means?\n",
    "\n",
    "Supervised learning algorithms include **classification** for categorical output and **regression** for continuous output. \n",
    "\n",
    "Note the difference between \"learning algorithm\" and \"algorithm\":\n",
    "+ \"algorithm\" is the function we discover that maps inputs to output\n",
    "+ \"learning algorithm\" is what we use, along with the data input, to build an algorithm\n",
    "+ For example, \"logistic regression\" is a learning algorithm. After training it with data, the specific formula with all coefficient values determined is an algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general work flow of supervised learning, as you've already experienced in your Modeling and Analytics course, and as we discussed in the first lecture of this course, involves five steps as follows:\n",
    "\n",
    "![Work Flow of Supervised Learning](image_BALifeCycle.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next walk through these five steps of supervised learning using the Python programming language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRFG14lxALtK"
   },
   "source": [
    "## Step 1\\. Data gathering ~~and wrangling~~\n",
    "\n",
    "The dataset we use is the **Lending Club** dataset. Refer to file \"LendingClub_description.pdf\" for details.\n",
    "\n",
    "As a very basic code, we won't get into data wrangling yet today. (We don't even know what to wrangle with yet.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyaYUjfIALtL"
   },
   "source": [
    "### Load the LendingClub dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8f5MpncALtL"
   },
   "outputs": [],
   "source": [
    "# Import Python packages as follows. This is analogous to library() in R.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# These two packages, NumPy and pandas, together allow us to manipulate data efficiently.\n",
    "# We'll study and use these two must-have data-science packages in the next few weeks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ku3ADolKALtM"
   },
   "outputs": [],
   "source": [
    "# Load the Lending Club dataset. Analogous to read.csv() in R.\n",
    "# In pandas, a data table is called a DataFrame. Thus we often use \"df\" to denote a data table.\n",
    "df = pd.read_csv('LendingClub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ku3ADolKALtM"
   },
   "outputs": [],
   "source": [
    "# Show on screen the first few records in this dataset. \n",
    "# Analogous to head() in R, yet now as a method of object \"df\".\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91gbrrTUALtK"
   },
   "source": [
    "### Data structure for supervised learning in Python\n",
    "\n",
    "Similar to R, in Python we usually expect data in a **table format** for predictive analytics, as shown above.\n",
    "+ each row is a **sample**/record, e.g. a customer\n",
    "+ each column is an input \"independent variable\"/attribute/**feature**/predictor, e.g., FICO score of a customer\n",
    "  + with the exception of one column being the output **target**/label /prediction/\"dependent variable\", e.g., \"not_fully_paid\" in the above dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ0EtSJ0od1l"
   },
   "source": [
    "Different from R, in Python we usually expect the inputs and output to be stored *separately* as follows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "679W8vTwALtM"
   },
   "outputs": [],
   "source": [
    "# Separate the dataset into a features matrix X and a target array y\n",
    "X = df.drop(columns=['not_fully_paid'])\n",
    "y = df['not_fully_paid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHk1vGmoiq1T"
   },
   "source": [
    "**Features matrix** (a.k.a. the inputs)\n",
    "\n",
    "The features matrix is often stored in a variable named `X`. The features matrix is assumed to be two-dimensional, with shape `[n_samples, n_features]`, and is most often contained in a NumPy `array` or a Pandas `DataFrame`. \n",
    "\n",
    "**Target array** (a.k.a. the output)\n",
    "\n",
    "In addition to the feature matrix `X`, we also work with a *target array* (or called *label array*) for supervised learning. By convention we call this array `y`. The target array is usually one dimensional, with length `n_samples`, and is generally contained in a NumPy `array` or Pandas `Series`. Values in the target array can be either continuous or discrete. The target array is what we want to *predict from the data*, such as whether a customer will default on a loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ0yWe9tALtO"
   },
   "source": [
    "### Data wrangling\n",
    "\n",
    "Is data wrangling important in real life? ***Absolutely yes.***\n",
    "\n",
    "Are we doing data wrangling today? Bravely no :) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwI5qL4XALtO"
   },
   "source": [
    "## Step 2\\. Exploratory data analysis (EDA)\n",
    "\n",
    "*In the real world, EDA and related data wrangling will likely take most of your time.* For today only, let's keep EDA to a bare minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbs6f2rqqW_0"
   },
   "outputs": [],
   "source": [
    "# How large is the dataset?\n",
    "df.shape\n",
    "# Analogous to dim() in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xta-InSpALtO"
   },
   "outputs": [],
   "source": [
    "# Any missing data?\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5afE1JdALtO"
   },
   "outputs": [],
   "source": [
    "# Summary statistics of each numerical column. Analogous to summary() in R.\n",
    "df.describe().loc[['mean','std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a histogram of annual income:\n",
    "np.exp(df.log_annual_inc).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnvSlWczALtP"
   },
   "source": [
    "Professor Geng thinks, *bravely*, that this dataset looks ready for predictive modeling. Geng thinks that no more EDA or data wrangling is needed. Let's assume Geng is right, and move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w-o7tjXALtP"
   },
   "source": [
    "## Step 3\\. Modeling\n",
    "\n",
    "Most traditional machine learning algorithms (as compared to deep learning) in Python are nicely bundled into the awesome **scikit-learn** package (we'll study this package more later in the semester). \n",
    "+ If anyone asks me why Python over R, scikit-learn is my top reason\n",
    "+ In Python coding this package is named `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULXr5Nj8ALtP"
   },
   "source": [
    "### Partition the data\n",
    "\n",
    "This is done using the `train_test_split()` function in the `sklearn` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohyQ4csDALtP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIkY7DyCALtP"
   },
   "outputs": [],
   "source": [
    "# reserve 20% dataset as testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HalFebjVALtQ"
   },
   "source": [
    "### Choose the learning algorithm\n",
    "\n",
    "The scikit-learn package offers numerous classification and regression learning algorithms, many of which are state-of-the-art choices. You can find them at (https://scikit-learn.org/stable/supervised_learning.html) -- this will be a topic of later weeks.\n",
    "\n",
    "As the target `not_fully_paid` is discrete: 1 for not fully paid, and 0 for fully paid, the Lending Club problem is a ***classification*** problem.\n",
    "+ Unlike R, there's no \"factor\" data type in Python. \n",
    "+ If the target is integer, it is assumed to be a classification problem. If the target is float, a regression problem.\n",
    "\n",
    "Let's try the ***logistic regression*** that we've learned last semester.\n",
    "+ Analogous to glm(..., family=\"binomial\") or train(..., method=\"glm\", family=\"binomial\") in R.\n",
    "\n",
    "(NOT required for today) scikit-learn is famous for providing high-quality documentations. For example, for logistic regression:\n",
    "+ You can find detailed explanation of the underlying statistical concepts at (https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) \n",
    "+ You can find detailed coding definitions and examples at (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnKu4Z_zALtQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZmL8XXgALtR"
   },
   "source": [
    "Since logistic regression cannot handle string data, let us drop column 'purpose' (but is this the proper way to handle?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls5hoM7vALtR"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['purpose'])\n",
    "X_test = X_test.drop(columns=['purpose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDcrm20dALtR"
   },
   "source": [
    "### Choose model hyperparameters\n",
    "\n",
    "Most learning algorithms, including `LogisticRegression` implemented in `sklearn`, have many parameters that need to be explicitly set before we can run them. They are referred to as **hyperparameters**. Often, how we choose the values of these hyperparameters -- called **hyperparameter tuning** -- will affect the performance of the eventual trained model.\n",
    "\n",
    "Example: The first hyperparameter of `LogisticRegression` is `penalty`. Our choice of its value will affect how complicated the trained algorithm is -- for example, how many features are eventually selected.\n",
    "\n",
    "For today, we'll ignore hyperparameter tuning, and blindly follow Geng's choice below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T945eoEeALtR"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epX_NhLwALtR"
   },
   "source": [
    "### Fit your model (a.k.a. train your model)\n",
    "\n",
    "Now let's fit/train our model. That is, plugging our Lending Club dataset into the chosen learning algorithm in order to generate a trained model (and pray it's good)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2rxtVTnALtR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax4ml1GmALtS"
   },
   "outputs": [],
   "source": [
    "# Run this code cell to see the coefficients of the trained model:\n",
    "logit_reg_coef = pd.DataFrame(model.coef_[0],index=X_train.columns,columns=['coefficient'])\n",
    "logit_reg_coef.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_DmAoDm_Eoa"
   },
   "source": [
    "Comment: In glm() in R, specifying the learning algorithm, specifying the hyperparameters, and training are all done in a single line of code. In scikit-learn, however, these are three separate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQZITIELALtS"
   },
   "source": [
    "One weakness of the scikit-learn package, as compared to R packages, is that it is more into prediction and less into the completeness of stats reporting. For example, `LogisticRegression` does not report the p-value. If you need it, try another package `statsmodels` as follows:\n",
    "```\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y_train,X_train)\n",
    "result=logit_model.fit()\n",
    "print(result.summary())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWJEnjl0ALtS"
   },
   "source": [
    "## Step 4\\. Evaluation\n",
    "\n",
    "Now it's time to see how well our trained algorithm performs. First, we apply the trained model to the testing data `X_test` to get predictions. This is done using the `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ik_2A4O_ALtS"
   },
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0QbRowIALtT"
   },
   "source": [
    "Second, we compare the predicted values in `y_predict` with the true values we already have in `y_test` using the `accuracy_score()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RH6rUuRzALtT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_predict).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHF9hyhbALtT"
   },
   "source": [
    "Does the accuracy of our trained model, as shown above, appear okay to you?\n",
    "\n",
    "Hint: try `1-y_test.mean()`, and think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJWBqXLSALtT"
   },
   "outputs": [],
   "source": [
    "1-y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ov7eSgMCALtT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"The confusion matrix is:\")\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylark9-6ALtT"
   },
   "source": [
    "***Chances are, we will not stumble upon an ideal trained model in our first try! Or the first many tries!*** Machine learning is almost always a repetitive process of observing the results, asking why, and forming our ideas on what next we should try, accordingly going back to data wrangling and EDA and model adjustment, and see if our ideas help or not. And repeat again if needed. This will occupy a big chunk of our time in the next few weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwkz2kfmNBPT"
   },
   "source": [
    "**So, what should we do next regarding this Lending Club analytics problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJ45ZMWEALtU"
   },
   "source": [
    "## Step 5\\. Deployment\n",
    "\n",
    "Once we have a champion model that we are happy with, we move it to deployment: using this trained model to automatically predict new data coming in, a.k.a. **scoring new data**, and to dispatch these predictions to decision makers. \n",
    "\n",
    "This typically involves converting markdown files to code files (i.e., from .ipynb file to .py file), setting up proper input/output pipelines, and task automation (e.g., running it at 8am everyday automatically). (Not required) You can start from this nice [guide](https://medium.com/@thabo_65610/three-ways-to-automate-python-via-jupyter-notebook-d14aaa78de9).\n",
    "\n",
    "Below we'll just do a simple prediction. As we don't have any new data. Let's use the first 5 records in the test dataset and pretend they are new loan applicants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loan_applicants = X_test.iloc[0:5]\n",
    "# Let's take a look at these \"new\" loan applicants:\n",
    "new_loan_applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of these 5 new applicants, what is our prediction?\n",
    "model.predict(new_loan_applicants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of these 5 new applicants, how confident are we regarding these predictions?\n",
    "model.predict_proba(new_loan_applicants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Class Exercise: Adapt the code for the Titanic dataset\n",
    "\n",
    "Now that you know the basic flow of supervised learning in Python, please try to modify the code so that it works on the Titanic dataset.\n",
    "+ Make a copy first. Name the copy \"titanic.ipynb\", and work on this copy.\n",
    "+ After each step, let's pause and discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What are we missing, before we can claim to be pros in Python-based analytics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ability to manipulate data freely using NumPy and pandas\n",
    "\n",
    "Recall that we wanted to: convert \"purpose\" to dummies, take a log of all dollar values, (in the Titanic dataset) convert Sex from male/female to 1/0, fill in missing values for Age, ...  \n",
    "\n",
    "**NumPy**: fast **array**-based computing\n",
    "+ You've already learned the basics of NumPy in our datacamp.com assignment 1 \"Introduction to Python\". \n",
    "+ See file \"NumPy.ipynb\" in Canvas -- a quick review/reference of key capabilities in NumPy relevant to data science.\n",
    "\n",
    "**pandas**: two data structures (**Series** and **DataFrame**), and a powerful set of tools for manipulating data (selecting, modifying, creating, grouping, merging, summarizing ...)\n",
    "+ ***datacamp.com assignment 2 \"Data Manipulation with pandas\" due by 11:59pm on 2/7/2023***\n",
    "  + When to start on this assignment? Two options:\n",
    "    + You can wait after we learn pandas in next week's class.\n",
    "    + Or, you can start on this datacamp.com assignment (which is actually an interactive course) now.\n",
    "  + You'll learn the basics of pandas in this assignment. It'll take you about 4 hours.\n",
    "  + ***Important***: make sure to use the link I provided in Lecture 1, i.e. (https://www.datacamp.com/groups/shared_links/d7926a437195d9598840413e575ac86e07de3de290ecb3f479eb007f93e611b7), to get to this assignment. Otherwise, you may not have free assess to it, or I won't see your completion within datacamp.com ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be able to implement most traditional supervised learning algorithms using scikit-learn\n",
    "\n",
    "Logistic regression? Check (sort of). What about decision tree? Random forest? SVM? Nearest Neighbors? ...\n",
    "\n",
    "**scikit-learn** (https://scikit-learn.org/) is THE best-known Python library for traditional (a.k.a. non-deep) machine learning. Advantages of scikit-learn include:\n",
    "\n",
    "- A [large selection](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) of machine learning algorithms\n",
    "  - and efficient implementation (a.k.a. fast) via NumPy and SciPy\n",
    "- A selection of metrics for measuring model performance\n",
    "- Excellent online documentation with awesome examples\n",
    "- A clean, uniform, and streamlined API. Once you learn how to code with one algorithm, switching to other algorithms is usually straightforward\n",
    "\n",
    "We will use two week (or more) to play with scikit-learn, including both the learning algorithms and the metrics, and to think how to drive better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not just to get results. To get MEANINGFUL and BETTER results.\n",
    "\n",
    "Are we happy with the logistic regression model we just built for the Lending Club dataset?\n",
    "\n",
    "Thoughts on the **metrics**: the metrics need to be meaningful for the business problems we try to help with. \n",
    "+ In a loan business, what are the reasonable metrics?\n",
    "\n",
    "Thoughts on the **learning algorithms** and related **data wrangling**: which learning algorithm to eventually choose? What hyperparameter values to set? What coresponding data wrangling should we try? \n",
    "+ Nowadays, these are largely empirical questions that we let computers try various alternatives, and then select the champion model.\n",
    "\n",
    "Thoughts on the **state-of-the-art techniques**\n",
    "+ Right now and for traditional learning, **boosting algorithms** are the king in terms of performance\n",
    "    + but ...\n",
    "+ **Deep learning**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPsLzAFjTtvdxK0bejJI4Db",
   "collapsed_sections": [],
   "mount_file_id": "1W35wFc9wPi-ica25OymjN_GHnzex_fPw",
   "name": "ML_2_BasicWorkFlow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
